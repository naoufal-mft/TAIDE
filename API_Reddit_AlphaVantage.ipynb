{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naoufal-mft/TRAIDE/blob/API/API_Reddit_AlphaVantage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analyse Combinée de Sentiment et de Données Financières des Actions Boursières**\n",
        "\n",
        "Ce notebook Jupyter (Colab) est conçu pour offrir une analyse complète des actions boursières, combinant l'analyse de sentiment des commentaires sur Reddit avec la collecte de données financières via Alpha Vantage. Les utilisateurs peuvent choisir n'importe quel symbole boursier pour obtenir une vue d'ensemble de la perception publique ainsi que des performances financières récentes de l'entreprise.\n",
        "\n",
        "# **Objectif**\n",
        "\n",
        "Ce notebook vise à fournir aux investisseurs, aux analystes financiers et aux amateurs de données une plateforme intégrée pour analyser à la fois les sentiments publics et les données financières d'une entreprise. Cela permet une prise de décision plus informée en incorporant des analyses qualitatives et quantitatives.\n",
        "\n",
        "# **Prérequis**\n",
        "\n",
        "Pour utiliser ce notebook, vous aurez besoin d'accès à des clés API valides pour Reddit et Alpha Vantage. Une familiarité avec Python, l'analyse de données, et les concepts financiers de base est recommandée pour maximiser les bénéfices de cet outil analytique.\n"
      ],
      "metadata": {
        "id": "-l9uskvA5bRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ces commandes installeront les bibliothèques asyncpraw pour l'interaction asynchrone avec l'API Reddit, pandas pour la manipulation et l'analyse des données, et praw pour l'interaction avec l'API Reddit de manière synchrone. Ces bibliothèques sont essentielles pour le traitement et l'analyse des données comme décrit dans les objectifs de votre notebook Colab.***"
      ],
      "metadata": {
        "id": "fV1febZD8lF6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzawQo05TLcp",
        "outputId": "ff0e8d1f-b45d-4e44-f24c-cac5017411f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting asyncpraw\n",
            "  Downloading asyncpraw-7.7.1-py3-none-any.whl (196 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/196.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m174.1/196.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.7/196.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<1 (from asyncpraw)\n",
            "  Downloading aiofiles-0.8.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (3.9.3)\n",
            "Collecting aiosqlite<=0.17.0 (from asyncpraw)\n",
            "  Downloading aiosqlite-0.17.0-py3-none-any.whl (15 kB)\n",
            "Collecting asyncprawcore<3,>=2.1 (from asyncpraw)\n",
            "  Downloading asyncprawcore-2.4.0-py3-none-any.whl (19 kB)\n",
            "Collecting update-checker>=0.18 (from asyncpraw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (4.0.3)\n",
            "Requirement already satisfied: typing_extensions>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from aiosqlite<=0.17.0->asyncpraw) (4.9.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from update-checker>=0.18->asyncpraw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2024.2.2)\n",
            "Installing collected packages: aiosqlite, aiofiles, update-checker, asyncprawcore, asyncpraw\n",
            "Successfully installed aiofiles-0.8.0 aiosqlite-0.17.0 asyncpraw-7.7.1 asyncprawcore-2.4.0 update-checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "pip install asyncpraw\n",
        "pip install pandas\n",
        "pip install praw"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "Ce script Python utilise asyncpraw (une version asynchrone de PRAW, l'API Python Reddit) pour collecter et analyser les commentaires de plusieurs subreddits spécifiques relatifs a un symbole boursier. Il vise à identifier les sentiments généraux (positifs, négatifs, neutres) exprimés dans ces commentaires sur une période de l'année passée, en utilisant l'analyseur de sentiments VADER de la bibliothèque NLTK.\n",
        "\n",
        "Fonctionnalités principales :\n",
        "1. Collecte des commentaires récents contenant le mot-clé \"NVDA\" dans les subreddits liés à la finance et à l'investissement.\n",
        "2. Analyse des sentiments de ces commentaires pour déterminer leur polarité (positif, négatif, neutre).\n",
        "3. Aggrégation des résultats par jour et détermination du sentiment dominant pour chaque jour.\n",
        "4. Sauvegarde des scores de sentiment quotidiens dans un fichier CSV pour une analyse ultérieure.\n",
        "\n",
        "Ce script démontre l'utilisation de l'asynchronie pour améliorer l'efficacité de la collecte de données sur Reddit, ainsi que l'application de l'analyse de sentiments NLP pour extraire des insights à partir de données textuelles brutes.\n",
        "\n",
        "Prérequis :\n",
        "- Python 3.6+\n",
        "- Bibliothèques : asyncio, asyncpraw, asyncprawcore, pandas, nltk (avec vader_lexicon téléchargé)\n",
        "\n",
        "Usage :\n",
        "Ce script est destiné à être exécuté comme un script autonome. Assurez-vous d'avoir configuré les identifiants de l'API Reddit (client_id, client_secret) avant l'exécution.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "TrTPTVLbzZCr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hds4eu3dMToO",
        "outputId": "d76c7752-8027-48d9-8c55-90704f15fed3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Daily sentiment scores successfully saved to CSV.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import asyncpraw\n",
        "from collections import Counter\n",
        "import datetime\n",
        "import asyncprawcore\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Téléchargement du lexique VADER pour l'analyse des sentiments\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Définition d'une fonction pour classer les scores de sentiment\n",
        "def classify_sentiment(score):\n",
        "    \"\"\"Classifie le score de sentiment en positif, négatif ou neutre.\"\"\"\n",
        "    if score > 0.05:  # Seuil pour positif\n",
        "        return 1\n",
        "    elif score < -0.05:  # Seuil pour négatif\n",
        "        return -1\n",
        "    else:\n",
        "        return 0  # Neutre\n",
        "\n",
        "async def collect_comments(reddit, subreddit_name, date_limite, sid, keyword):\n",
        "    \"\"\"Collecte les commentaires d'un subreddit spécifique contenant un mot-clé et analyse leur sentiment.\"\"\"\n",
        "    subreddit = await reddit.subreddit(subreddit_name)\n",
        "    comments_data = []\n",
        "\n",
        "    # Parcourir les soumissions récentes\n",
        "    async for submission in subreddit.new(limit=None):  # Enlever la limite ou la mettre très haute\n",
        "        submission_date = datetime.datetime.utcfromtimestamp(submission.created_utc).date()\n",
        "        if submission_date >= date_limite:\n",
        "            try:\n",
        "                await submission.load()\n",
        "                if submission.comments:\n",
        "                    await submission.comments.replace_more(limit=0)\n",
        "                    for comment in submission.comments.list():\n",
        "                        if comment and comment.body and keyword.lower() in comment.body.lower():\n",
        "                            sentiment_score = sid.polarity_scores(comment.body)['compound']\n",
        "                            comments_data.append({\n",
        "                                'Date': submission_date,\n",
        "                                'Comment': comment.body,\n",
        "                                'Sentiment': sentiment_score\n",
        "                            })\n",
        "                            # Ajouter un délai pour respecter la limite de requêtes\n",
        "                            await asyncio.sleep(1)\n",
        "            except asyncprawcore.exceptions.TooManyRequests as e:\n",
        "                wait_time = e.response.headers.get(\"Retry-After\", 60)  # Utiliser 60 secondes par défaut si l'header n'est pas présent\n",
        "                print(f\"Rate limit exceeded. Waiting for {wait_time} seconds.\")\n",
        "                await asyncio.sleep(int(wait_time))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing comments: {e}\")\n",
        "\n",
        "    return comments_data\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Point d'entrée principal pour la collecte et l'analyse des sentiments des commentaires Reddit.\"\"\"\n",
        "    # Configuration de l'instance Reddit avec asyncpraw\n",
        "    reddit = asyncpraw.Reddit(\n",
        "        client_id=\"A5JdkIx6Fd_92LzdvaRhLg\",\n",
        "        client_secret=\"mHYkASS6LWg0EObYuy745JXQmU6s0w\",\n",
        "        user_agent=\"APITUTO\"\n",
        "    )\n",
        "\n",
        "    date_limite = datetime.datetime.utcnow().date() - datetime.timedelta(days=365)  # Collecter des données de l'année passée\n",
        "    sid = SentimentIntensityAnalyzer()  # Initialisation de l'analyseur de sentiments VADER\n",
        "    subreddits = [\"stocks\", \"finance\", \"investing\", \"wallstreetbets\", \"StockMarket\"]\n",
        "    keyword = \"\"  # Symbole boursier à rechercher\n",
        "\n",
        "    all_comments = []\n",
        "    for subreddit_name in subreddits:\n",
        "        comments = await collect_comments(reddit, subreddit_name, date_limite, sid, keyword)\n",
        "        all_comments.extend(comments)\n",
        "\n",
        "    # Création d'un DataFrame à partir de all_comments\n",
        "    df_comments = pd.DataFrame(all_comments)\n",
        "    # Appliquer la fonction de classification à chaque score de sentiment\n",
        "    df_comments['Sentiment_Class'] = df_comments['Sentiment'].apply(classify_sentiment)\n",
        "\n",
        "    # Aggrégation des scores de sentiment par jour\n",
        "    df_comments['Date'] = pd.to_datetime(df_comments['Date'])\n",
        "    df_daily_dominant = df_comments.groupby('Date')['Sentiment_Class'].agg(lambda group: Counter(group).most_common(1)[0][0]).reset_index(name='Dominant_Sentiment')\n",
        "\n",
        "    # Sauvegarde des résultats dans un fichier CSV\n",
        "    df_daily_dominant.to_csv('NVDA_daily_sentiment_scores.csv', index=False)\n",
        "    print(\"Daily sentiment scores successfully saved to CSV.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette commande installe la bibliothèque alpha_vantage, qui vous permet d'accéder aux données financières et aux indicateurs de marché via l'API Alpha Vantage. Alpha Vantage offre des données gratuites sur les actions, les cryptomonnaies, les devises (forex), et plus encore, ce qui est particulièrement utile pour les analyses financières et les projets d'investissement."
      ],
      "metadata": {
        "id": "4dBkO9YgHdjo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eI9m2P7tmNiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a81c722-e5fe-4aa2-bf58-9734b902850d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting alpha_vantage\n",
            "  Downloading alpha_vantage-2.3.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from alpha_vantage) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from alpha_vantage) (2.31.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->alpha_vantage) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->alpha_vantage) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->alpha_vantage) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->alpha_vantage) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->alpha_vantage) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->alpha_vantage) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->alpha_vantage) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->alpha_vantage) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->alpha_vantage) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->alpha_vantage) (2024.2.2)\n",
            "Installing collected packages: alpha_vantage\n",
            "Successfully installed alpha_vantage-2.3.1\n"
          ]
        }
      ],
      "source": [
        "pip install alpha_vantage"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Ce script effectue les opérations suivantes :\n",
        "\n",
        "1. **Importation des bibliothèques nécessaires** : `pandas` pour la manipulation des données, `alpha_vantage.timeseries` pour accéder aux données boursières via l'API Alpha Vantage, et `datetime` pour gérer les dates.\n",
        "2. **Configuration de la clé API** : Remplacez `''` par votre propre clé API obtenue d'Alpha Vantage.\n",
        "3. **Définition de la période de récupération des données** : Calcul de la date de début pour récupérer les données des 14 derniers mois.\n",
        "4. **Récupération des données** : Pour chaque symbole dans la liste `symbols`, le script récupère les données boursières quotidiennes, les reformate pour une meilleure lisibilité et les filtre à partir de la date de début calculée.\n",
        "5. **Préparation des données pour l'analyse** : Réinitialisation de l'index pour transformer les dates en colonne, ce qui facilite les manipulations et analyses ultérieures.\n",
        "6. **Sauvegarde des données** : Enregistrement du DataFrame contenant toutes les données récupérées dans un fichier CSV nommé `donnees_boursieres_PLUG.csv`.\n",
        "\n",
        "Avant de lancer ce script, assurez-vous que votre environnement Python dispose des packages nécessaires (`pandas` et `alpha_vantage`) et que vous avez configuré correctement votre clé API Alpha Vantage. Ce script est un excellent point de départ pour l'analyse financière et peut être adapté pour inclure d'autres symboles boursiers ou modifier la période de récupération des données.\n"
      ],
      "metadata": {
        "id": "WNI6ISSO1Xio"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HtlZVSPAl88u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "import datetime\n",
        "\n",
        "key = '4FRYV9E3HHKX8SNR'  # Remplacez par votre clé API\n",
        "\n",
        "# Liste des symboles boursiers à analyser\n",
        "symbols = ['']\n",
        "\n",
        "# Initialisation d'un DataFrame vide pour stocker toutes les données\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "# Calcul de la date de début pour la récupération des données (14 mois avant aujourd'hui)\n",
        "start_date = (datetime.datetime.now() - datetime.timedelta(days=365 + (365/12)*2)).strftime('%Y-%m-%d')\n",
        "\n",
        "# Initialisation de l'objet TimeSeries avec la clé API\n",
        "ts = TimeSeries(key, output_format='pandas')\n",
        "\n",
        "for symbol in symbols:\n",
        "    # Récupération des données boursières quotidiennes du symbole\n",
        "    data, meta = ts.get_daily(symbol, outputsize='full')\n",
        "    # Renommage des colonnes pour plus de clarté\n",
        "    data.rename(columns={'1. open': 'open', '2. high': 'high', '3. low': 'low', '4. close': 'close', '5. volume': 'volume'}, inplace=True)\n",
        "\n",
        "    # Conversion de l'index en datetime et filtrage des données depuis la date de début\n",
        "    data.index = pd.to_datetime(data.index)\n",
        "    data = data[data.index >= start_date]\n",
        "\n",
        "    # Réinitialisez l'index pour avoir la date comme colonne\n",
        "    data.reset_index(inplace=True)\n",
        "    data.rename(columns={'index': 'Date'}, inplace=True)\n",
        "\n",
        "    # Concaténation des données récupérées avec le DataFrame global\n",
        "    all_data = pd.concat([all_data, data], axis=0)\n",
        "\n",
        "all_data.to_csv('donnees_boursieres_PLUG.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "Ce script Python est conçu pour fusionner deux ensembles de données distincts : les données financières quotidiennes et les scores de sentiment quotidien basés sur l'analyse de commentaires sur Reddit. L'objectif est d'explorer la corrélation potentielle entre les mouvements du marché boursier et le sentiment public envers l'entreprise, tel qu'il est exprimé à travers les médias sociaux.\n",
        "\n",
        "Les étapes principales du script sont les suivantes :\n",
        "1. Chargement des données financières à partir d'un fichier CSV, avec conversion des dates au format datetime pour une manipulation aisée.\n",
        "2. Chargement des données de sentiment à partir d'un autre fichier CSV, avec une attention similaire aux formats de date.\n",
        "3. Fusion des deux ensembles de données sur la base des dates, en utilisant une fusion externe pour s'assurer que toutes les dates présentes dans les deux ensembles sont incluses, même si l'une des parties n'a pas de données correspondantes pour certaines dates.\n",
        "4. Traitement des valeurs manquantes dans les scores de sentiment, en remplaçant les valeurs NaN par 0 pour indiquer un sentiment neutre par défaut.\n",
        "5. Conversion du type de la colonne de sentiment en entier pour faciliter les analyses ultérieures.\n",
        "6. Sélection des colonnes pertinentes pour l'analyse et sauvegarde du DataFrame résultant dans un nouveau fichier CSV pour une utilisation ultérieure.\n",
        "\n",
        "Ce processus permet une analyse intégrée des performances boursières d'NVDA et de la perception publique, offrant des insights potentiels pour les investisseurs, les analystes et les amateurs de données.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "-NHm84fq21nE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEeqOXbusKnU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger les données financières\n",
        "df_financial = pd.read_csv('donnees_boursieres_NVDA.csv')\n",
        "df_financial['date'] = pd.to_datetime(df_financial['date'])\n",
        "df_financial.rename(columns={'date': 'Date'}, inplace=True)\n",
        "\n",
        "# Charger les données de sentiment\n",
        "df_sentiment = pd.read_csv('NVDA_daily_sentiment_scores.csv')\n",
        "df_sentiment['Date'] = pd.to_datetime(df_sentiment['Date'])\n",
        "\n",
        "# Fusionner les données financières avec les données de sentiment, en utilisant une fusion externe pour inclure toutes les dates\n",
        "df_combined = pd.merge(df_financial, df_sentiment, on='Date', how='outer')\n",
        "\n",
        "# Remplir les valeurs NaN de 'Dominant_Sentiment' avec 0\n",
        "df_combined['Dominant_Sentiment'] = df_combined['Dominant_Sentiment'].fillna(0)\n",
        "\n",
        "# Assurer que 'Dominant_Sentiment' est de type entier\n",
        "df_combined['Dominant_Sentiment'] = df_combined['Dominant_Sentiment'].astype('Int64')\n",
        "\n",
        "# Sélectionner les colonnes requises, y compris 'Dominant_Sentiment' maintenant en entiers\n",
        "df_final = df_combined[['Date', 'open', 'high', 'low', 'close', 'volume', 'Dominant_Sentiment']]\n",
        "\n",
        "# Sauvegarder le DataFrame final dans un nouveau fichier CSV\n",
        "df_final.to_csv('NVDA_combined_financial_sentiment.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "Ce script asynchrone est conçu pour analyser le sentiment public envers une action souhaitée en collectant et en évaluant les commentaires des utilisateurs sur différents subreddits liés à la finance et à l'investissement. En utilisant la bibliothèque asyncpraw pour interagir avec l'API Reddit de manière asynchrone, le script extrait les commentaires récents qui mentionnent \"le symbol\". Chaque commentaire est ensuite analysé avec l'analyseur de sentiments VADER de la bibliothèque NLTK pour déterminer s'il exprime un sentiment positif, négatif, ou neutre.\n",
        "\n",
        "Principales fonctionnalités du script :\n",
        "1. Collecte asynchrone des commentaires mentionnant \"NVDA\" dans les subreddits spécifiés sur une période d'un an.\n",
        "2. Analyse des sentiments de chaque commentaire à l'aide de VADER pour classer les sentiments en positif, négatif ou neutre.\n",
        "3. Agrégation des données pour calculer le nombre total de commentaires, ainsi que le nombre et le pourcentage de commentaires positifs et négatifs par jour.\n",
        "4. Sauvegarde des résultats dans un fichier CSV pour une analyse ultérieure, offrant une vue d'ensemble de la perception publique de NVDA sur Reddit.\n",
        "\n",
        "Ce script peut servir d'outil d'analyse pour les investisseurs, les analystes financiers, ou les passionnés de la technologie et de la finance, en fournissant des insights précieux sur le sentiment du marché envers NVIDIA, potentiellement utiles pour la prise de décision en matière d'investissement.\n",
        "\n",
        "Prérequis :\n",
        "- Python 3.6 ou plus récent.\n",
        "- Bibliothèques Python : asyncio, asyncpraw, pandas, nltk (avec le lexique VADER téléchargé).\n",
        "\n",
        "Note : Assurez-vous d'avoir une clé API valide pour Reddit pour utiliser asyncpraw et de respecter les limites de requêtes lors de l'exécution de ce script.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "CdxNG6J74Krm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD73MUAtxM9q",
        "outputId": "cc7f1fc3-cac8-4764-8fab-244168632b00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: received 429 HTTP response\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import asyncpraw\n",
        "from collections import Counter\n",
        "import datetime\n",
        "import asyncprawcore\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "def classify_sentiment(score):\n",
        "    if score > 0.1:\n",
        "        return 1  # Positive sentiment\n",
        "    elif score < -0.05:\n",
        "        return -1  # Negative sentiment\n",
        "    else:\n",
        "        return 0  # Neutral sentiment\n",
        "\n",
        "\n",
        "async def collect_comments(reddit, subreddit_name, date_limite, sid, keyword):\n",
        "    subreddit = await reddit.subreddit(subreddit_name)\n",
        "    comments_data = []\n",
        "\n",
        "    async for submission in subreddit.new(limit=None):  # Enlever la limite ou la mettre très haute\n",
        "        submission_date = datetime.datetime.utcfromtimestamp(submission.created_utc).date()\n",
        "        if submission_date >= date_limite:\n",
        "            try:\n",
        "                await submission.load()\n",
        "                if submission.comments:\n",
        "                    await submission.comments.replace_more(limit=0)\n",
        "                    for comment in submission.comments.list():\n",
        "                        if comment and comment.body and keyword.lower() in comment.body.lower():\n",
        "                            sentiment_score = sid.polarity_scores(comment.body)['compound']\n",
        "                            sentiment_class = classify_sentiment(sentiment_score)  # Utiliser la fonction pour classer le sentiment\n",
        "                            comments_data.append({\n",
        "                                'Date': submission_date,\n",
        "                                'Comment': comment.body,\n",
        "                                'Sentiment': sentiment_score,\n",
        "                                'Sentiment_Class': sentiment_class  # Ajouter la classification de sentiment\n",
        "                           })\n",
        "\n",
        "            except Exception as e:  # Ici, on ajoute un bloc except pour gérer les exceptions\n",
        "                print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "    return comments_data\n",
        "\n",
        "async def main():\n",
        "    reddit = asyncpraw.Reddit(\n",
        "        client_id=\"A5JdkIx6Fd_92LzdvaRhLg\",\n",
        "        client_secret=\"mHYkASS6LWg0EObYuy745JXQmU6s0w\",\n",
        "        user_agent=\"APITUTO\"\n",
        "    )\n",
        "\n",
        "    date_limite = datetime.datetime.utcnow().date() - datetime.timedelta(days=365)  # 1 an\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    subreddits = [\"stocks\", \"finance\", \"investing\", \"wallstreetbets\", \"StockMarket\"]\n",
        "    keyword = \"NVDA\"  # Symbole boursier à rechercher\n",
        "\n",
        "    all_comments = []\n",
        "    for subreddit_name in subreddits:\n",
        "        comments = await collect_comments(reddit, subreddit_name, date_limite, sid, keyword)\n",
        "        all_comments.extend(comments)\n",
        "\n",
        "    # Création d'un DataFrame à partir de all_comments\n",
        "    df_comments = pd.DataFrame(all_comments)\n",
        "\n",
        "    # Convertir 'Date' en datetime et 'Sentiment_Class' selon la classification\n",
        "    df_comments['Date'] = pd.to_datetime(df_comments['Date'])\n",
        "\n",
        "    # Calculer le nombre total de commentaires par jour\n",
        "    daily_counts = df_comments.groupby('Date')['Sentiment_Class'].count().reset_index(name='Total_Comments')\n",
        "\n",
        "    # Calculer le nombre de commentaires positifs et négatifs par jour\n",
        "    positive_counts = df_comments[df_comments['Sentiment_Class'] == 1].groupby('Date')['Sentiment_Class'].count().reset_index(name='Positive_Comments')\n",
        "    negative_counts = df_comments[df_comments['Sentiment_Class'] == -1].groupby('Date')['Sentiment_Class'].count().reset_index(name='Negative_Comments')\n",
        "\n",
        "    # Fusionner les données\n",
        "    daily_data = daily_counts.merge(positive_counts, on='Date', how='left').merge(negative_counts, on='Date', how='left').fillna(0)\n",
        "\n",
        "    # Calculer les pourcentages\n",
        "    daily_data['Positive_Percentage'] = round((daily_data['Positive_Comments'] / daily_data['Total_Comments']) * 100, 2)\n",
        "    daily_data['Negative_Percentage'] = round((daily_data['Negative_Comments'] / daily_data['Total_Comments']) * 100, 2)\n",
        "\n",
        "\n",
        "    # Sauvegarder dans un CSV si nécessaire\n",
        "    daily_data.to_csv('NVDA_percentage.csv', columns=['Date', 'Total_Comments', 'Positive_Percentage', 'Negative_Percentage'], index=False, float_format='%.2f')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  await main()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZDdO3W+bnpYZ/OcNT4irT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}